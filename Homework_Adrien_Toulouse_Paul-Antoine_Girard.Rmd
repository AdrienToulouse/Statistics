---
title: 'MAP 531: Homework'
author: 'Paul-Antoine GIRARD & Adrien TOULOUSE'
output:
  pdf_document: default
header-includes: \usepackage{dsfont}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Problem 1: Estimating parameters of a Poisson distribution

We recall that the Poisson distribution with parameter $\theta > 0$ has a pdf given by ($p(\theta, k), k \in \mathbb{N})$ w.r.t the counting measure on $\mathbb{N}$:  
$$p(\theta, k) = e^{-\theta} \frac{\theta^k}{k!}$$

### Question 1

The poisson distribution is a discrete distribution since it has a countable number of possible values ($\mathbb{N}$).

In statistics, we use this distribution to compute the probability of a given number of (rare) events in a time period.

For example a poisson distribution can model:

* The number of patients arriving in an emergency room between 9 and 10am.

* The number of network failures per day.

* In quality control, the number of manufacturing defect.


### Question 2

We assume that $\mathbb{X}$ follows a Poisson distribution with parameter $\theta > 0$.  

We will use the fact that $e^{\theta} = \sum_{i=0}^{\infty} (\frac{\theta^{i}}{i!}), \forall \theta \in \mathbb{R}$
$$
\mathbb{E}[\mathbb{X}] = \sum_{i=0}^{\infty} (i * p(\theta, i)) = \sum_{i=0}^{\infty} (i*e^{-\theta} \frac{\theta^{i}}{i!}) = \theta * e^{-\theta}\sum_{i=1}^{\infty} (\frac{\theta^{i-1}}{(i-1)!}) = \theta * e^{-\theta} \sum_{i=0}^{\infty} (\frac{\theta^{i}}{i!}) = \theta * e^{-\theta} * e^{\theta} = \theta  
$$

$$
\mathbb{E}[\mathbb{X}^2] = \sum_{i=0}^{\infty} (i^2 * p(\theta, i)) = \sum_{i=0}^{\infty} (i^2*e^{-\theta} \frac{\theta^{i}}{i!}) = \theta * e^{-\theta}\sum_{i=1}^{\infty} (i\frac{\theta^{i-1}}{(i-1)!}) = \theta * e^{-\theta}\sum_{i=0}^{\infty} ((i+1)\frac{\theta^{i}}{i!})
$$
$$
= \theta * e^{-\theta}[\sum_{i=0}^{\infty} (i\frac{\theta^{i}}{i!}) + \sum_{i=0}^{\infty} (\frac{\theta^{i}}{i!})] = \theta * e^{-\theta}[\theta \sum_{i=0}^{\infty} (\frac{\theta^{i}}{i!}) + e^{\theta}] = \theta * e^{-\theta}[\theta * e^{\theta} + e^{\theta}] = \theta (\theta + 1)
$$

$$
\mathbb{V} (\mathbb{X}) = \mathbb{E}[\mathbb{X}^2] - \mathbb{E}[\mathbb{X}]^2 = \theta (\theta + 1) - \theta^2 = \theta
$$

### Question 3

We are provided with n independent observations of a Poisson random variable of parameter $\theta \in \Theta = \mathbb{R_+^*}$. Our observations are $X_k \sim Pois (\theta), \forall k \in {1, ..., n}$.  
The corresponding statistical model is: $$\mathcal{M}^n = (\mathbb{N}^n, \mathcal{P}(\mathbb{N}^n),\ \{\mathbb{P}^n _{\theta},\ \theta \in\Theta \})$$
with $\mathbb{P}^n _{\theta} = \mathbb{P} _{\theta} \otimes ... \otimes \mathbb{P} _{\theta}$ (n times)  
We are trying to estimate the parameter $\theta$.

### Question 4

The likelihood function is the function on $\theta$ that makes our n observations most likely.

Using the independance of the $X_k$:
$$
l(\theta) = \prod_{k=1}^{n} e^{-\theta} \frac{\theta^{X_{k}}}{X_{k}!}
$$

$$
L(\theta) = log(l(\theta)) = \sum_{k=1} ^{n}(- \theta + X_k log(\theta) - log(X_k!)) = - n \theta + log(\theta) \sum_{k=1}^{n}X_{k} - \sum_{k=1}^{n}log(X_{k}!)
$$

By derivating with respect to $\theta$, we have:

$$
L'(\theta) = -n +\frac{\sum_{k=1}^{n}X_{k}}{\theta}
$$
$$
L''(\theta) = - \frac{\sum_{k=1}^{n}X_{k}}{\theta^2} < 0
$$
Since, the second derivative of the log-likelihood function is negative, the function is concave and admits a global maximum given by:
$$
L'(\theta) = 0 \Leftrightarrow -n +\frac{\sum_{k=1}^{n}X_{k}}{\theta} = 0 \Leftrightarrow \hat\theta_{MLE} = \overline{X}
$$

So, the maximum likelihood estimator is:
$$
\hat\theta_{MLE} = \overline{X}
$$

### Question 5

Since the $X_k$ are iid, we have that:
$$
\mathbb{E}[\overline{X}] = \frac{1}{n} \sum _{k = 1} ^{n} \mathbb{E} [X_k] = \mathbb{E} [X_1] = \theta
$$

$$
\mathbb{V}(\overline{X}) = \frac{1}{n^2} \sum _{k = 1} ^{n} \mathbb{V} (X_k) = \frac{1}{n} \mathbb{V} [X_1] = \frac {\theta} {n}
$$
Applying the central limit theorem, we have that $\sqrt{n}(\hat\theta_{MLE}-\theta)$ converges towards a Gaussian $\mathcal{N}(0,\theta)$.

### Question 6

The weak law of large numbers gives us that: $$\hat\theta_{MLE} \overset{p} {\to} \theta$$ 
By continuous mapping, we have: $$\sqrt{\hat\theta_{MLE}} \overset{p} {\to} \sqrt{\theta}$$
Then, by applying Slutsky's theorem, we have that: $$\sqrt{n}\frac{(\hat\theta_{MLE}-\theta)}{\sqrt{\hat\theta_{MLE}}} \overset{d} {\to} \mathcal{N}(0,1)$$

Now, let's check this result in R by simulating 1000 times our random variable $\sqrt{n}\frac{(\hat\theta_{MLE}-\theta)}{\sqrt{\hat\theta_{MLE}}}$ with a sample size of 100:
```{r}
estim <- function(x, theta){
  n <- length(x)
  est <- sqrt(n) * (mean(x) - theta) / sqrt(mean(x))
  return(est)
  }
```

```{r}
set.seed(23)
Nattempts = 1e3
nsample = 100
theta = 3

samples <- lapply(1:Nattempts, function(i) rpois(nsample, theta))
realisations <- sapply(samples, function(x) estim(x, theta))

hist(realisations, probability = TRUE)
d = density(realisations, kernel='gaussian')
lines(d, col = 'red')
```

The histogram confirms what we found theoretically. In fact, by plotting the density associated to the histogram we can observe a curve that represents a gaussian distribution. It is symetric around its expectation that seems to be zero. So, we can conclude that the random variable $\sqrt{n}\frac{(\hat\theta_{MLE}-\theta)}{\sqrt{\hat\theta_{MLE}}}$ follows a standard gaussian distribution.  

```{r}
qqnorm(realisations)
qqline(realisations)
```

The Q-Q plot compares the theoritical quantiles of a standard gaussian distribution to the ones of our estimated distribution. We can observe that the points approximately lie on the line y = x, so the distributions compared are similar and this plot also confirms that the random variable $\sqrt{n}\frac{(\hat\theta_{MLE}-\theta)}{\sqrt{\hat\theta_{MLE}}}$ follows a standard gaussian distribution.

### Question 7

Let $Z_n = \sqrt{n} \frac{(\hat\theta_{MLE}-\theta)}{\sqrt{\hat\theta_{MLE}}}$ be our random variable.

Denote $z_{\alpha}$ the $\alpha$-quantile for the standard Normal distribution for $\alpha \in (0,\ 1)$.

$$
\lim \limits_{n \rightarrow + \infty} \mathbb{P} (-z_{1-\alpha/2} \leq Z_n \leq z_{1-\alpha/2}) \ge 1- \alpha \Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P}(-z_{1-\alpha/2} \sqrt{\frac{\hat\theta_{MLE}}{n}} \leq \hat\theta_{MLE} - \theta \leq z_{1-\alpha/2}\sqrt{\frac{\hat\theta_{MLE}}{n}}) \ge 1- \alpha
$$

For $\alpha \in (0, 1)$,  an asymptotic confidence interval of level $\alpha$ for $\theta$ is therefore:

$$
[\hat \theta_{MLE} - z_{1-\alpha/2} \frac{\sqrt{\hat\theta_{MLE}}}{\sqrt{n}} ;\ \hat \theta_{MLE} + z_{1-\alpha/2} \frac{\sqrt{\hat\theta_{MLE}}} {\sqrt{n}}]
$$

### Question 8

We apply the $\delta$-method with $g(x) = 2 \sqrt{x}$  
We have: $g'(x) = \frac {1} {\sqrt{x}}$  
So,
$$
\sqrt{n} (g(\hat \theta_{MLE}) - g(\theta)) \overset{d} {\to} \mathcal{N}(0,\ g'(\theta)^2 \times \theta) \Leftrightarrow \sqrt{n} (g(\hat \theta_{MLE}) - g(\theta)) \overset{d} {\to} \mathcal{N}(0,1)
$$
$$\Leftrightarrow \sqrt{n} (2 \sqrt{\hat \theta_{MLE}} - 2 \sqrt{\theta}) \overset{d} {\to} \mathcal{N}(0,1)$$

### Question 9

Let $W_n = \sqrt{n} (2 \sqrt{\hat\theta_{MLE}} - 2 \sqrt{\theta})$ be our random variable.

We know by the last question that $W_n \overset{d} {\to} \mathcal{N}(0,1)$.
$$
\lim \limits_{n \rightarrow + \infty} \mathbb{P} (-z_{1-\alpha/2} \leq W_n \leq z_{1-\alpha/2}) \geq 1- \alpha \Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P}(- \frac {z_{1-\alpha/2}} {2 \sqrt{n}} \leq \sqrt{\hat\theta_{MLE}} - \sqrt{\theta} \leq \frac {z_{1-\alpha/2}} {2 \sqrt{n}}) \geq 1- \alpha
$$

$$
 \Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P} (\sqrt{\hat\theta_{MLE}} - \frac {z_{1-\alpha/2}} {2 \sqrt{n}} \leq \sqrt{\theta} \leq \sqrt{\hat\theta_{MLE}} + \frac {z_{1-\alpha/2}} {2 \sqrt{n}}) \geq 1- \alpha
$$
When n goes towards infinity, $\frac {z_{1-\alpha/2}} {2 \sqrt{n}}$ goes to 0. Since  $\sqrt{\hat\theta_{MLE}}$ is positive, there exists a $n_0$ such that $\forall n \geq n_0$, $\sqrt{\hat\theta_{MLE}} - \frac {z_{1-\alpha/2}} {2 \sqrt{n}}$ is positive and we can take the squares in the inequality without changing the order of the inequalities:

$$
\Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P} ((\sqrt{\hat\theta_{MLE}} - \frac {z_{1-\alpha/2}} {2 \sqrt{n}})^2 \leq \theta \leq (\sqrt{\hat\theta_{MLE}} + \frac {z_{1-\alpha/2}} {2 \sqrt{n}})^2) \geq 1- \alpha
$$


For $\alpha \in (0, 1)$,  an asymptotic confidence interval for $\theta$ of level $\alpha$ is therefore:
$$
[(\sqrt{\hat\theta_{MLE}} - \frac {z_{1-\alpha/2}} {2 \sqrt{n}})^2 ;\ (\sqrt{\hat\theta_{MLE}} + \frac {z_{1-\alpha/2}} {2 \sqrt{n}})^2]
$$

### Question 10

Based on the first moment of a poisson distribution, we easily have that:
$$\hat \theta_{MME} = \overline{X}$$

We can remark that $\hat \theta_{MME} = \hat \theta_{MLE}$

Based on the second moment of a poisson distribution, we have:
$$n^{-1} \sum _{k=1} ^{n} X_k^2 = \hat \theta_{2} (\hat \theta_{2} + 1) = \hat \theta_{2}^2 + \hat \theta_{2}$$

We can now construct many different estimators by replace one of the $\hat \theta_2$ by $\hat \theta_1$ or we can inverse the function $h(x) = x(x + 1)$ to find an estimator of $\theta$.

The inverse function of $h$ on $\mathbb{R}_+^*$ is $h^{-1} (x) = \frac {1}{2} [- 1 + \sqrt{4 x + 1}]$ and this gives us another estimator of $\theta$:
$$\hat \theta_{2} = \frac {1}{2} [- 1 + \sqrt{(4 n^{-1} \sum _{k=1} ^{n} X_k^2) + 1}]$$

### Question 11

$\mathbb{E} [\hat\theta_{MLE}] = \frac{1}{n} \sum_{k=1}^{n} \mathbb{E}[X_k]$ by linearity of the expectation.
So, $$\mathbb{E} [\hat\theta_{MLE}] = \frac{1}{n} * n * \theta = \theta$$

Therefore, $\hat\theta_{MLE}$ is an unbiased estimator of $\theta$, ie. $b_\theta{^*}(\hat\theta_{MLE}) = 0$

$\mathbb{V} (\hat\theta_{MLE}) = \frac{1}{n^2} \sum_{i=1}^{n} \mathbb{V}(X_i)$ by independance of the $X_k$.
$$\mathbb{V} (\hat\theta_{MLE}) = \frac{1}{n^2} * n * \theta = \frac{\theta}{n}$$

The quadratic risk Q is:
$$Q = b_\theta{^*}(\hat\theta_{MLE})^2 + \mathbb{V^*} (\hat\theta_{MLE}) = 0 + \frac{\theta}{n} = \frac{\theta}{n}$$

### Question 12

$\hat\theta_{MLE}$ is an unbiased estimator. So the Cramer-Rao bound is given by:  
$$\frac{1}{I_n(\theta^*)} = \frac{1}{\mathbb{E} [-L''(\theta^*)]}$$

By derivating the log-likelihood function with respect to $\theta$, we have:
$$L'(\theta^*) = -n + \frac{\sum_{i=1}^{n} X_k}{\theta}$$
$$-L''(\theta^*) = \frac{\sum_{i=1}^{n} X_k}{\theta^2}$$

Therefore,
$$\mathbb{E} [-L''(\theta^*)] = \frac{\sum_{i=1}^{n} \mathbb{E}[X_k]}{\theta^2} = \frac{n}{\theta}$$

Finally,
$$\frac{1}{I_n(\theta^*)} = \frac{\theta}{n} = \mathbb{V} (\hat\theta_{MLE})$$

We can conclude that our estimator $\hat\theta_{MLE}$ is efficient.

### Question 13

$$
\hat\theta_{2} = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X_n})^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \theta + \theta - \overline{X_n})^2 = \frac{1}{n} \sum_{i=1}^{n} [(X_i - \theta)^2 + (\theta - \overline{X_n} )^2 +2(X_i - \theta)(\theta - \overline{X_n})]
$$
$$
= \frac{1}{n} \sum_{i=1}^{n} (X_i - \theta)^2 + (\theta - \overline{X_n} )^2  + \frac{2}{n}(\theta - \overline{X_n}) \sum_{i=1}^{n}(X_i - \theta) \\
= \frac{1}{n} \sum_{i=1}^{n} (X_i - \theta)^2 + (\theta - \overline{X_n} )^2 + 2(\theta - \overline{X_n}) (\overline{X_n} - \theta )
$$
$$
= \frac{1}{n} \sum_{i=1}^{n} (X_i - \theta)^2 - (\theta - \overline{X_n})^2
$$

### Question 14

$$
\mathbb{E}[(\theta - \overline{X_n})^2] = \mathbb{E}[\theta^2 - 2\theta\overline{X_n} + \overline{X_n}^2] = \theta^2 - 2\theta\mathbb{E}[\overline{X_n}] + \mathbb{E}[\overline{X_n^2}]
$$
$$
=- \theta^2 + \mathbb{V}(\overline{X_n}) + \mathbb{E}[\overline{X_n}^2] = - \theta^2 + \frac{\theta}{n} + \theta^2 = \frac{\theta}{n}
$$

$$
\mathbb{E}[\hat\theta_2] = \mathbb{E} [\frac{1}{n} \sum_{i=1}^{n} (X_i - \theta)^2 - (\theta - \overline{X_n})^2] = \frac{1}{n} \sum_{i=1}^{n}\mathbb{E}[(X_i - \theta)^2] - \mathbb{E}[(\theta - \overline{X_n})^2]
$$
$$= \frac{1}{n} \sum_{i=1}^{n}\mathbb{V}(X_i) - \frac{\theta}{n} = \theta(1-\frac{1}{n})$$
Therefore the bias is:
$$b_{\hat\theta_2} =  -\frac{\theta}{n}$$

We can get an unbiased estimator $\hat\theta_3$ by defining $\hat\theta_3 = (1-\frac{1}{n}) ^{-1} \hat\theta_2$  

### Question 15

Using the previous questions, we know that:
$$\hat\theta_{2} = \frac{1}{n} \sum_{i=1}^{n} (X_i - \theta)^2 - (\theta - \overline{X_n})^2$$
therefore, we have:
$$
\sqrt{n} (\hat\theta_{2} - \theta) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} (X_i - \theta)^2 - \sqrt{n} (\theta - \overline{X_n})^2 - \sqrt{n} \theta = \sqrt{n} (\overline{Y_n} - \theta) -  \sqrt{n} (\theta - \overline{X_n})^2
$$
where:
$$\forall i \in[\![1,n]\!], Yi = (X_i - \theta)^2 $$
$$\bar{Y_n} = \frac{1}{n} \sum_{i=1}^{n}Y_i $$

Since:
$$
\mathbb{E}[Y_i] = \mathbb{V}(X_i) = \theta
$$

$$
\mathbb{V}(Y_i) = 2\theta^2 + \theta
$$

We can apply the central limit theorem, and we have that: $$\sqrt{n} (\overline{Y_n} - \theta) \overset{d} {\to} \mathcal{N}(0,2\theta^2 + \theta)$$

We also have the following equalities:
$$
\sqrt{n} (\theta - \overline{X_n})^2 = \sqrt{n} (\overline{X_n} - \theta)^2 = \sqrt{n} (\overline{X_n} - \theta)(\overline{X_n} - \theta)
$$

By applying the central limit theorem, we now have: $$\sqrt{n} (\overline{X_n} - \theta) \overset{d} {\to} \mathcal{N}(0,\theta)$$  
On the other hand, applying the law of large numbers gives us: $$(\theta - \overline{X_n})\overset{p} {\to} 0$$

Then, by applying Slutsky's theorem, we have that $\sqrt{n} (\theta - \overline{X_n})^2$ converges in distribution towards the constant 0. Therefore, it converges in probability towards 0.

Now, we can apply Slutsky's theorem to $\sqrt{n} (\overline{Y_n} - \theta) -  \sqrt{n} (\theta - \overline{X_n})^2$ which gives us finally that: $$\sqrt{n} (\hat\theta_{2} - \theta) \overset{d} {\to} \mathcal{N}(0,2\theta^2 + \theta)$$  


We can now compute another asymptotic confidence interval centered in $\hat \theta_2$.

We know by the first part of question that $\frac{\sqrt{n} (\hat\theta_{2} - \theta)}{\sqrt{2\theta^2 + \theta}} \overset{d} {\to} \mathcal{N}(0,1)$.

Let's use $\hat\theta_{2}$ as an estimator of $\theta$ for the denominator.  
First, in order to apply slutsky's theorem, we need to prove that $\hat\theta_{2} \overset{p} {\to} \theta$.

Let $\epsilon > 0$.
By Chebyshev's inequality,
$$
P(\mid \hat\theta_{2} - \theta \mid > \epsilon) \leq \frac{Var(\hat\theta_{2} - \theta)} {\epsilon^2} = \frac{\theta^2 + \theta} {n\epsilon^2} \underset{n \to + \infty} {\to} 0
$$

So, we have that $\hat\theta_{2} \overset{p} {\to} \theta$ and by continuous mapping, $$\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}} \overset{p} {\to} \sqrt{2\theta^2 + \theta}$$

Then, by Slutsky's theorem, we have that $V_n =\sqrt{n}\frac{(\hat\theta_{2}-\theta)}{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}}$ converges in law towards a gaussian $\mathcal{N}(0,1)$, and we can use it as pivotal quantity to find a confidence interval for $\theta$.

$$
\lim \limits_{n \rightarrow + \infty} \mathbb{P} (-z_{1-\alpha/2} \leq V_n \leq z_{1-\alpha/2}) \geq 1- \alpha \Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P}(-z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}} \leq \hat\theta_{2} - \theta \leq z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}}) \geq 1- \alpha
$$

$$
 \Leftrightarrow \lim \limits_{n \rightarrow + \infty} \mathbb{P} (\hat\theta_{2} - z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}} \leq \theta \leq \hat\theta_{2} + z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}}) \geq 1- \alpha
$$

For $\alpha \in (0, 1)$,  an asymptotic confidence interval for $\theta$ of level $\alpha$ is therefore:
$$
[\hat\theta_{2} - z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}};\ \hat\theta_{2} + z_{1-\alpha/2} \frac{\sqrt{2\hat\theta_{2}^2 + \hat\theta_{2}}} {\sqrt{n}}]
$$

This asymptotic interval has a bigger range compared to the first one

Furthermore, we have that:
$$var(\hat\theta_{2}) = \frac{2\theta^2 + \theta} {n} = \frac{\theta (2\theta + 1)} {n} = (2\theta + 1) Var(\hat \theta_{MLE})$$

### Question 16

Let $s \in \mathbb{R}$.
The probability generating function of the Poisson distribution is given by:
$$
G_\mathbb{X} (s) = \mathbb{E}[exp(s\mathbb{X})] = \sum _{k=0} ^{\infty} e^{ks} e^{-\theta} \frac{\theta^k}{k!} = e^{-\theta} \sum _{k=0} ^{\infty} \frac{(\theta e^s)^k}{k!} = e^{-\theta} e^{\theta e^{s}} = e^{\theta(e^{s}-1)}
$$

In order to compute the first and second moment of the Poisson distribution, we can now use the moment generating function.
Let's compute its first and second order derivatives.

$$G_\mathbb{X}' (s) = \theta e^{s} e^{\theta (e^{s}-1)}$$
$$G_\mathbb{X}'' (s) = \theta [e^{s} e^{\theta (e^{s}-1)} + \theta e^{2s} e^{\theta (e^{s}-1)}] = \theta e^{s} [e^{\theta (e^{s}-1)} + \theta e^{s} e^{\theta (e^{s}-1)}]$$

Then, we have:
$$\mathbb{E} [\mathbb{X}] = G_\mathbb{X}' (0) = \theta$$
$$\mathbb{E} [\mathbb{X}^2] = G_\mathbb{X}'' (0) = \theta(1 + \theta)$$
$$\mathbb{V}(\mathbb{X}) = \mathbb{E} [\mathbb{X}^2] - \mathbb{E} [\mathbb{X}]^2 = \theta(1 + \theta) - \theta^2 = \theta$$

We will now show that: $\mathbb{V}[(\mathbb{X}_i - \theta)^2] = 2 \theta^2 + \theta$

We use the following equalities:
$$G_\mathbb{X}^{(3)} (s) = (1 + 3 \theta e^{s} + \theta^2 e^{2s}) \theta e^{s + \theta (e^{s}-1)}$$
$$G_\mathbb{X}^{(4)} (s) = (1 + \theta^3 e^{3s} + 6 \theta^2 e^{2s} + 7\theta e^{s}) \theta e^{s + \theta (e^{s}-1)}$$
$$\mathbb{E} [\mathbb{X}^3] = G_\mathbb{X}^{(3)} (0) = \theta + 3\theta^2 + \theta^3$$
$$\mathbb{E} [\mathbb{X}^4] = G_\mathbb{X}^{(4)} (0) = \theta^4 + 6\theta^3 + 7\theta^2 + \theta$$

Finally, we have that:
$$
\mathbb{V}[(\mathbb{X}_i - \theta)^2] = \mathbb{E} [(\mathbb{X} - \theta)^4] - \mathbb{E} [(\mathbb{X} - \theta)^2]^2 = \mathbb{E} [\mathbb{X}^4] - 4 \theta \mathbb{E} [\mathbb{X}^3] + 6 \theta^2 \mathbb{E} [\mathbb{X}^2]  - 4 \theta^3 \mathbb{E} [\mathbb{X}] + \theta^4 - Var (\mathbb{X})^2
$$
$$
= \theta^4 + 6\theta^3 + 7\theta^2 + \theta - 4\theta (\theta + 3\theta^2 + \theta^3) + 6 \theta^2 (\theta +\theta^2) - 4\theta^4  + \theta^4 - \theta^2 = 2 \theta^2 + \theta
$$
\newpage

# Problem 2: Analysis of the USJudgeRatings dataset

This exercise is open. You are asked to use the tools we have seen together to analyze the USJudgeRatings data set. This data set is provided in the package datasets. Your analysis should be reported here and include:

* an introduction
* a general description of the data
* the use of descriptive statistics
* the use of all techniques we have seen together that might be relevant
* a conclusion

Overall, your analysis, including the graphs and the codes should not exceed 15 pages in pdf.

## Introduction

We are given to analyse a dataset, named USJudgeratings, containing various ratings of state judges in the US Superior Court made by lawyers.
The different variables given help us to determine if a judge is worthy staying in the US Superior Court or not.
We will start by doing a general description of the data and applying descriptive statistics to better apprehend the data.

## General description

We start by uploading our data.

```{r upload data}
data(USJudgeRatings)
```

First, let's see how the dataset is organized.  
```{r Introduction}
str(USJudgeRatings)
```
The data is stored in a dataframe nad we can observe that all the variables are numeric.

```{r}
dim(USJudgeRatings)
```
We are provided with n = 43 observations and p = 12 quantitative variables.

We can have a full view of the dataset by using the kable function:
```{r view}
library(knitr)
library(kableExtra)
kable(USJudgeRatings, 'latex', caption = "Ratings of US judges", booktabs = T) %>%
  kable_styling(latex_options = "striped", font_size = 6)
```

An observation in this dataset represents the different ratings received by a judge (given by his name) in the US Superior Court.  
In order to study this dataset, we will first define properly what each variable means.

```{r Colnames}
colnames(USJudgeRatings)
```
The variables are:

* *CONT* : The number of contacts of the lawyer with judge.
* *INTG* : The judicial integrity of the judge.
* *DMNR* : Demeanor of the judge.
* *DILG* : Diligence of the judge.
* *CFMG* : Case flow managed by the judge.
* *DECI* : Prompt decisions taken by the judge.
* *PREP* : How the judge is prepared trial.
* *FAMI* : The judge's familiarity with law.
* *ORAL* : The judge's sound oral rulings.
* *WRIT* : The judge's sound written rulings.
* *PHYS* : The judge's physical ability.
* *RTEN* : Scaling if the judge is worthy to retain in the US Superior court.

## Descriptive dataset analysis

### Let's inspect the dataframe for missing values, outliers and errors:

```{r missing values?}
sum(is.na(USJudgeRatings))
```
There are no missing values in the dataframe.

```{r Summary}
summary(USJudgeRatings)
```
All the variables (except the variable CONT) are ranged between 0 and 10. Furthermore, we can observe that each variable admits a median that is close to the mean. For example, the median for the INTG variable is 8.021 and its median is equal to 8.100. So the variables seem to follow symetric distributions.  
We will now take a look on the boxplots of the variables to see if there is outliers and errors, and to compare the interquartile ranges.

```{r boxplot}
Outvals = boxplot(USJudgeRatings)
```

We observe the presence of outliers for 10 of the 12 variables (with larger values for CONT and with lower values for the other variables). Most of the variables have only one or two outliers, except the variables PHYS and RTEN, which have four outliers each. We are not provided with extra information and nothing indicated that these outliers correspond to mistakes. Thus, we will assume that they aren't mistakes and keep them in our analysis.  
Note that the PHYS variable has a small interquartile range compare to the other variables. That means that all the judges seem to have a good physical ability (superior to 7) except the four outliers.  
Also, the four outliers for the RTEN variable mean that the laywers think that these four judges should not stay in the US Superior court. Therefore, we need to see if there a link between the other variable and the variable RTEN.

Let's take a closer look at these outliers.
```{r max_CONT}
max(USJudgeRatings$CONT)
rownames(USJudgeRatings)[which.max(USJudgeRatings$CONT)]
```
The judge with the biggest number of contacts of lawyer is judge Callahan with a a number of 10.6 contacts.

```{r highest rating}
max(USJudgeRatings$RTEN)
rownames(USJudgeRatings)[which.max(USJudgeRatings$RTEN)]
```
The judge with the highest rating for worthiness of retention is judge Rubinow with a rating of 9.2.  
We can take a look at his other ratings.
```{r}
USJudgeRatings[which.max(USJudgeRatings$RTEN),]
```

```{r lowest rating}
min(USJudgeRatings$RTEN)
rownames(USJudgeRatings)[which.min(USJudgeRatings$RTEN)]
```
The judge with the lowest rating for worthiness of retention is judge Bracken with a rating of 4.8.
We can also take a look at his other ratings.
```{r}
USJudgeRatings[which.min(USJudgeRatings$RTEN),]
```
By comparing the ratings of these two judges, we can observe that the judge with the highest RTEN ratings has better ratings in the other variables except for the variable CONT. Thus, there seems to be a correlation between the RTEN variable and the others (except CONT). We will look deeper into that correlation later in this report.

### Let's see the distribution of the variable

```{r histograms1}
par(mfrow=c(2,2))
hist(USJudgeRatings$CONT, probability= TRUE, main="Histogram of CONT", xlab="CONT")
d = density(USJudgeRatings$CONT, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$INTG, probability= TRUE, main="Histogram of INTG", xlab="INTG")
d = density(USJudgeRatings$INTG, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$DMNR, probability= TRUE, main="Histogram of DMNR", xlab="DMNR")
d = density(USJudgeRatings$DMNR, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$DILG, probability= TRUE, main="Histogram of DILG", xlab="DILG")
d = density(USJudgeRatings$DILG, kernel = 'o', bw = 0.3)
lines(d, col="red")
```
These histograms give us an approximation of the distribution followed by these variables.  
For example, for the CONT variable, we can see a bell with its highest point near the mean (near 7.4) of the sample. So we can assume that this variable follow a gaussian distribution. However, we also see the two outliers on the histogram and that modify a bit the distribution and we lose a bit of symmetry.
For the DMNR variable,


```{r histograms2}
par(mfrow=c(2,2))
hist(USJudgeRatings$CFMG, probability= TRUE, main="Histogram of CFMG", xlab="CFMG")
d = density(USJudgeRatings$CFMG, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$DECI, probability= TRUE, main="Histogram of DECI", xlab="DECI")
d = density(USJudgeRatings$DECI, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$PREP, probability= TRUE, main="Histogram of PREP", xlab="PREP")
d = density(USJudgeRatings$PREP, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$FAMI, probability= TRUE, main="Histogram of FAMI", xlab="FAMI")
d = density(USJudgeRatings$FAMI, kernel = 'o', bw = 0.3)
lines(d, col="red")
```
Explain

```{r histograms3}
par(mfrow=c(2,2))
hist(USJudgeRatings$ORAL, probability= TRUE, main="Histogram of ORAL", xlab="ORAL")
d = density(USJudgeRatings$ORAL, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$WRIT, probability= TRUE, main="Histogram of WRIT", xlab="WRIT")
d = density(USJudgeRatings$WRIT, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$PHYS, probability= TRUE, main="Histogram of PHYS", xlab="PHYS")
d = density(USJudgeRatings$PHYS, kernel = 'o', bw = 0.3)
lines(d, col="red")
hist(USJudgeRatings$RTEN, probability= TRUE, main="Histogram of RTEN", xlab="RTEN")
d = density(USJudgeRatings$RTEN, kernel = 'o', bw = 0.3)
lines(d, col="red")
```
To comment...

### Let's analyze the correlation between the variables

First, we can take a look at the covariance matrix, and the standard deviation of eache variable.
```{r cov}
round(var(USJudgeRatings), 2)
```

```{r st deviation}
round(sqrt(diag(var(USJudgeRatings))),2)
print('The smallest standard deviation is: ')
min(round(sqrt(diag(var(USJudgeRatings))),2))
print('The largest standard deviation is: ')
max(round(sqrt(diag(var(USJudgeRatings))),2))
```
We find that the variables DMNR and RTEN have the largest standard deviation, while the DECI variable has the smallest.  

Let's measure the correlations between the 11 first variables and the variable RTEN to see if there is a link between the ratings and if the laywers think a judge is worthy staying in the US Superior court.  
Thus, we will plot the scatter plots of the variables relative to the RTEN variable and we will use the correlation functions.

```{r linear relationships between variables 1}
round(cor(USJudgeRatings),2)
```

```{r linear relationships between variables 2}
library(corrplot)
corrplot(cor(USJudgeRatings))
```

```{r pairs}

pairs(USJudgeRatings, gap=1/5, lower.panel = panel.smooth, upper.panel = NULL, row1attop=FALSE)
{r pairs.default(), fig.width = 5, fig.asp = .62}
```

All the variables have a strong positive correlation two by two except the variable CONT which is not correlated to all the other variables.
The number of contacts of a lawyer with the judge doesn't seem to explain the ratings received by the judge.


```{r ecdf}
par(mfrow=c(1,2))
plot(ecdf(USJudgeRatings$CONT), verticals = TRUE, do.points = FALSE, main = "ECDF CONT")
plot(ecdf(USJudgeRatings$RTEN), verticals = TRUE, do.points = FALSE, main = "ECDF RTEN")
```

```{r QQ plot}
qqnorm(USJudgeRatings$RTEN)
qqline(USJudgeRatings$RTEN)
```
The QQ plots suggests that the RTEN variable seems to follow a Gaussian distribution except for lower values.

```{r}
library(e1071)
kurtosis(USJudgeRatings$RTEN)
skewness(USJudgeRatings$RTEN)
```
Skewness and kurtosis figures for the retention variable are both between -1 and 1 which indicates no substantial skewness or kurtosis.

### Conclusion

Our analysis has
